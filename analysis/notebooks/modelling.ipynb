{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/gabriel/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DataPrep\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "# Modelling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109265/2255767118.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/raw/B2W-Reviews01.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>site_category_lv1</th>\n",
       "      <th>site_category_lv2</th>\n",
       "      <th>review_title</th>\n",
       "      <th>overall_rating</th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "      <th>reviewer_birth_year</th>\n",
       "      <th>reviewer_gender</th>\n",
       "      <th>reviewer_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:11:28</td>\n",
       "      <td>d0fb1ca69422530334178f5c8624aa7a99da47907c44de...</td>\n",
       "      <td>132532965</td>\n",
       "      <td>Notebook Asus Vivobook Max X541NA-GO472T Intel...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Informática</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>Bom</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Estou contente com a compra entrega rápida o ú...</td>\n",
       "      <td>1958.0</td>\n",
       "      <td>F</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 00:13:48</td>\n",
       "      <td>014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...</td>\n",
       "      <td>22562178</td>\n",
       "      <td>Copo Acrílico Com Canudo 500ml Rocie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utilidades Domésticas</td>\n",
       "      <td>Copos, Taças e Canecas</td>\n",
       "      <td>Preço imbatível, ótima qualidade</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>M</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 00:26:02</td>\n",
       "      <td>44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...</td>\n",
       "      <td>113022329</td>\n",
       "      <td>Panela de Pressão Elétrica Philips Walita Dail...</td>\n",
       "      <td>philips walita</td>\n",
       "      <td>Eletroportáteis</td>\n",
       "      <td>Panela Elétrica</td>\n",
       "      <td>ATENDE TODAS AS EXPECTATIVA.</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>M</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 00:35:54</td>\n",
       "      <td>ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...</td>\n",
       "      <td>113851581</td>\n",
       "      <td>Betoneira Columbus - Roma Brinquedos</td>\n",
       "      <td>roma jensen</td>\n",
       "      <td>Brinquedos</td>\n",
       "      <td>Veículos de Brinquedo</td>\n",
       "      <td>presente mais que desejado</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>F</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 01:00:28</td>\n",
       "      <td>7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...</td>\n",
       "      <td>131788803</td>\n",
       "      <td>Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...</td>\n",
       "      <td>lg</td>\n",
       "      <td>TV e Home Theater</td>\n",
       "      <td>TV</td>\n",
       "      <td>Sem duvidas, excelente</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes</td>\n",
       "      <td>A entrega foi no prazo, as americanas estão de...</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>M</td>\n",
       "      <td>MG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       submission_date                                        reviewer_id  \\\n",
       "0  2018-01-01 00:11:28  d0fb1ca69422530334178f5c8624aa7a99da47907c44de...   \n",
       "1  2018-01-01 00:13:48  014d6dc5a10aed1ff1e6f349fb2b059a2d3de511c7538a...   \n",
       "2  2018-01-01 00:26:02  44f2c8edd93471926fff601274b8b2b5c4824e386ae4f2...   \n",
       "3  2018-01-01 00:35:54  ce741665c1764ab2d77539e18d0e4f66dde6213c9f0863...   \n",
       "4  2018-01-01 01:00:28  7d7b6b18dda804a897359276cef0ca252f9932bf4b5c8e...   \n",
       "\n",
       "  product_id                                       product_name  \\\n",
       "0  132532965  Notebook Asus Vivobook Max X541NA-GO472T Intel...   \n",
       "1   22562178               Copo Acrílico Com Canudo 500ml Rocie   \n",
       "2  113022329  Panela de Pressão Elétrica Philips Walita Dail...   \n",
       "3  113851581               Betoneira Columbus - Roma Brinquedos   \n",
       "4  131788803  Smart TV LED 43\" LG 43UJ6525 Ultra HD 4K com C...   \n",
       "\n",
       "    product_brand      site_category_lv1       site_category_lv2  \\\n",
       "0             NaN            Informática                Notebook   \n",
       "1             NaN  Utilidades Domésticas  Copos, Taças e Canecas   \n",
       "2  philips walita        Eletroportáteis         Panela Elétrica   \n",
       "3     roma jensen             Brinquedos   Veículos de Brinquedo   \n",
       "4              lg      TV e Home Theater                      TV   \n",
       "\n",
       "                       review_title  overall_rating recommend_to_a_friend  \\\n",
       "0                               Bom               4                   Yes   \n",
       "1  Preço imbatível, ótima qualidade               4                   Yes   \n",
       "2      ATENDE TODAS AS EXPECTATIVA.               4                   Yes   \n",
       "3        presente mais que desejado               4                   Yes   \n",
       "4            Sem duvidas, excelente               5                   Yes   \n",
       "\n",
       "                                         review_text  reviewer_birth_year  \\\n",
       "0  Estou contente com a compra entrega rápida o ú...               1958.0   \n",
       "1  Por apenas R$1994.20,eu consegui comprar esse ...               1996.0   \n",
       "2  SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               1984.0   \n",
       "3  MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               1985.0   \n",
       "4  A entrega foi no prazo, as americanas estão de...               1994.0   \n",
       "\n",
       "  reviewer_gender reviewer_state  \n",
       "0               F             RJ  \n",
       "1               M             SC  \n",
       "2               M             SP  \n",
       "3               F             SP  \n",
       "4               M             MG  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/B2W-Reviews01.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing\n",
    "# Apply  regex\n",
    "\n",
    "def apply_regex(corpus, regex):\n",
    "    corpus = [re.sub(regex, ' ', x) for x in corpus]\n",
    "    return corpus\n",
    "\n",
    "def multiple_regex(corpus, regex_list):\n",
    "    # Lowcase\n",
    "    corpus = corpus.apply(lambda x: x.lower())\n",
    "    # Negation\n",
    "    corpus = [re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' não ', r) for r in corpus]\n",
    "    # Basix regex\n",
    "    for regex in regex_list:\n",
    "        corpus = apply_regex(corpus, regex)\n",
    "    return corpus\n",
    "\n",
    "def stemmer(corpus):\n",
    "    stemmer = RSLPStemmer()\n",
    "    stemmetized = []\n",
    "    for sentence in corpus:\n",
    "        words = [stemmer.stem(word) for word in sentence]\n",
    "        sentence = ' '.join(words)\n",
    "        stemmetized.append(sentence)\n",
    "    return corpus\n",
    "\n",
    "def lemmatizer(corpus):\n",
    "    lemmatized = []\n",
    "    for sentence in corpus:\n",
    "        doc = nlp(sentence)\n",
    "        lemmat = ' '.join([token.lemma_ for token in doc])\n",
    "        lemmatized.append(lemmat)\n",
    "    return lemmatized\n",
    "\n",
    "def data_cleaning(df):\n",
    "    df = df.copy()\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Feature engineering\n",
    "    df['review_text'] = df['review_title'] + ' ' + df['review_text']\n",
    "    cols=['submission_date', 'reviewer_id', 'product_id', 'product_name', 'product_brand',\n",
    "          'site_category_lv1', 'site_category_lv2', 'overall_rating', 'reviewer_birth_year',\n",
    "          'reviewer_gender', 'reviewer_state', 'review_title']\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    # Noise removal\n",
    "    regex_list = [r'www\\S+', r'http\\S+', r'@\\S+', r'#\\S+', r'[0-9]+', r'\\W', r'\\s+', r'[ \\t]+$']\n",
    "    df['review_text'] = multiple_regex(df['review_text'], regex_list)\n",
    "    df['review_text'] = lemmatizer(df['review_text'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML classification models\n",
    "### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127982\n",
      "CPU times: user 14min 6s, sys: 623 ms, total: 14min 6s\n",
      "Wall time: 14min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create Feature and Label sets\n",
    "df_ml = data_cleaning(df.copy())\n",
    "X = df_ml['review_text']\n",
    "y = df_ml['recommend_to_a_friend']\n",
    "\n",
    "print(len(df_ml))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming avg time: ~ 7 min\n",
    "\n",
    "Lemma avg time: ~ 14 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recommend_to_a_friend</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>bom estar contente com o compra entregar rápid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>preço imbatível ótimo qualidade por apenas r e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>atender todo o expectativa superar em agilidad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>presente mais que desejado meu filho amar pare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>sem duvida excelente o entrega ser em o prazo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recommend_to_a_friend                                        review_text\n",
       "0                   Yes  bom estar contente com o compra entregar rápid...\n",
       "1                   Yes  preço imbatível ótimo qualidade por apenas r e...\n",
       "2                   Yes  atender todo o expectativa superar em agilidad...\n",
       "3                   Yes  presente mais que desejado meu filho amar pare...\n",
       "4                   Yes  sem duvida excelente o entrega ser em o prazo ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_ml.to_csv('../data/clean/lemma.csv') # type: ignore\n",
    "df_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 102385\n",
      "Testing Data Size:  25597\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # type: ignore\n",
    "print('Training Data Size:', len(X_train))\n",
    "print('Testing Data Size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, min_df=2, max_df=0.95, ngram_range=(1,3), stop_words=pt_stopwords)\n",
    "tfidf_vect = TfidfVectorizer(lowercase=True, min_df=2, max_df=0.95, ngram_range=(1,3), stop_words=pt_stopwords)\n",
    "\n",
    "X1 = X_train.copy()\n",
    "X2 = X_train.copy()\n",
    "\n",
    "X_train_tfidf = tfidf_vect.fit_transform(X1)\n",
    "X_train_count= count_vect.fit_transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 28 ms, total: 2.42 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)\n",
    "# Form a prediction set\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "predictions = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_test, predictions):\n",
    "    print('f1 score: ',metrics.f1_score(y_test, predictions, pos_label=\"Yes\"))\n",
    "    print('\\n', '------------------', '\\n')\n",
    "    print(metrics.classification_report(y_test, predictions))\n",
    "    print('\\n', '------------------', '\\n')\n",
    "    print(metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9475507954008505\n",
      "\n",
      " ------------------ \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.84      0.85      6599\n",
      "         Yes       0.95      0.95      0.95     18998\n",
      "\n",
      "    accuracy                           0.92     25597\n",
      "   macro avg       0.90      0.90      0.90     25597\n",
      "weighted avg       0.92      0.92      0.92     25597\n",
      "\n",
      "\n",
      " ------------------ \n",
      "\n",
      "[[ 5551  1048]\n",
      " [  950 18048]]\n"
     ]
    }
   ],
   "source": [
    "evaluate(y_test, predictions) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data rows = 102385\n",
      "Number of features in bag of words = 237116\n"
     ]
    }
   ],
   "source": [
    "# Number of features in bag of words = 232110\n",
    "\n",
    "print(f'Number of data rows = {X2.shape[0]}') # type: ignore\n",
    "print(f'Number of features in bag of words = {X_train_count.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ngram_range=(1,3) despite adding important data also makes the matrix really sparse which can lead to failed convergence specially when using CV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Models\n",
    "\n",
    "**goal:** minimize false positives (due to imbalanced dataset) and to target negative reviews from users\n",
    "\n",
    "**metric:** precision\n",
    "\n",
    "**models:** since most sentiment trained models are in english this option will not be considered a first go. Also due to the limited ammount of data deep learning probably won't fit very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lin_svc = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, min_df=5, max_df=0.85, stop_words=pt_stopwords, max_features=5000)),\n",
    "                     ('lin_svc', LinearSVC())])\n",
    "\n",
    "text_rf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, min_df=5, max_df=0.85, stop_words=pt_stopwords, max_features=5000)),\n",
    "                     ('rf', RandomForestClassifier())])\n",
    "\n",
    "text_nb = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, min_df=5, max_df=0.85, stop_words=pt_stopwords, max_features=5000)),\n",
    "                     ('nb', MultinomialNB())])\n",
    "\n",
    "text_lr = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, min_df=5, max_df=0.85, stop_words=pt_stopwords, max_features=5000)),\n",
    "                     ('lr', LogisticRegression())])\n",
    "\n",
    "\n",
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3), (2, 3)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "}\n",
    "\n",
    "pipelines = [text_rf, text_nb, text_lr, text_lin_svc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time= 1.9min\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time= 1.9min\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time= 2.0min\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time= 2.0min\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time= 2.1min\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time= 2.1min\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time= 2.1min\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time= 2.2min\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time= 2.0min\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time= 2.1min\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time= 2.0min\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time= 2.0min\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time= 2.4min\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time= 2.5min\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time= 2.5min\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time= 2.6min\n",
      "\n",
      " Current Pipeline:  RandomForestClassifier() \n",
      "\n",
      "Best precision: 0.9329416298358307\n",
      "Best Parameters: {'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}\n",
      "\n",
      " Precision on training data:  0.9995\n",
      "Precision on training data:  0.9362 \n",
      "\n",
      "Confusion Matrix : \n",
      "[[ 5368  1231]\n",
      " [  932 18066]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.81      0.83      6599\n",
      "         Yes       0.94      0.95      0.94     18998\n",
      "\n",
      "    accuracy                           0.92     25597\n",
      "   macro avg       0.89      0.88      0.89     25597\n",
      "weighted avg       0.91      0.92      0.91     25597\n",
      "\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   4.8s\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   4.7s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   5.4s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   5.7s\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=  10.0s\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=  10.0s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=   9.9s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=  10.1s\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  15.5s\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  16.0s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  15.2s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  15.6s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  11.9s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  12.1s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.5s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.4s\n",
      "\n",
      " Current Pipeline:  MultinomialNB() \n",
      "\n",
      "Best precision: 0.9408213225878218\n",
      "Best Parameters: {'tfidf__ngram_range': (1, 3), 'tfidf__use_idf': True}\n",
      "\n",
      " Precision on training data:  0.9443\n",
      "Precision on training data:  0.9422 \n",
      "\n",
      "Confusion Matrix : \n",
      "[[ 5514  1085]\n",
      " [ 1314 17684]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.81      0.84      0.82      6599\n",
      "         Yes       0.94      0.93      0.94     18998\n",
      "\n",
      "    accuracy                           0.91     25597\n",
      "   macro avg       0.87      0.88      0.88     25597\n",
      "weighted avg       0.91      0.91      0.91     25597\n",
      "\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   4.5s\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   4.6s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   5.1s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   5.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=  10.6s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=  10.6s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=  11.0s\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=  11.2s\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  15.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  16.3s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  15.9s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  16.3s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.1s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  11.7s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.4s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  11.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Current Pipeline:  LogisticRegression() \n",
      "\n",
      "Best precision: 0.9423498510824917\n",
      "Best Parameters: {'tfidf__ngram_range': (1, 2), 'tfidf__use_idf': False}\n",
      "\n",
      " Precision on training data:  0.9491\n",
      "Precision on training data:  0.9452 \n",
      "\n",
      "Confusion Matrix : \n",
      "[[ 5551  1048]\n",
      " [  912 18086]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.86      0.84      0.85      6599\n",
      "         Yes       0.95      0.95      0.95     18998\n",
      "\n",
      "    accuracy                           0.92     25597\n",
      "   macro avg       0.90      0.90      0.90     25597\n",
      "weighted avg       0.92      0.92      0.92     25597\n",
      "\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   4.6s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   4.4s\n",
      "[CV] END ....tfidf__ngram_range=(1, 1), tfidf__use_idf=False; total time=   4.6s\n",
      "[CV] END .....tfidf__ngram_range=(1, 1), tfidf__use_idf=True; total time=   5.4s\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=   9.4s\n",
      "[CV] END .....tfidf__ngram_range=(1, 2), tfidf__use_idf=True; total time=   9.5s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=   9.5s\n",
      "[CV] END ....tfidf__ngram_range=(1, 2), tfidf__use_idf=False; total time=   9.5s\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  15.7s\n",
      "[CV] END .....tfidf__ngram_range=(1, 3), tfidf__use_idf=True; total time=  15.9s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  15.4s\n",
      "[CV] END ....tfidf__ngram_range=(1, 3), tfidf__use_idf=False; total time=  16.4s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  12.0s\n",
      "[CV] END ....tfidf__ngram_range=(2, 3), tfidf__use_idf=False; total time=  12.0s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.3s\n",
      "[CV] END .....tfidf__ngram_range=(2, 3), tfidf__use_idf=True; total time=  12.5s\n",
      "\n",
      " Current Pipeline:  LinearSVC() \n",
      "\n",
      "Best precision: 0.9419372934123866\n",
      "Best Parameters: {'tfidf__ngram_range': (1, 3), 'tfidf__use_idf': False}\n",
      "\n",
      " Precision on training data:  0.9534\n",
      "Precision on training data:  0.9467 \n",
      "\n",
      "Confusion Matrix : \n",
      "[[ 5582  1017]\n",
      " [  944 18054]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.86      0.85      0.85      6599\n",
      "         Yes       0.95      0.95      0.95     18998\n",
      "\n",
      "    accuracy                           0.92     25597\n",
      "   macro avg       0.90      0.90      0.90     25597\n",
      "weighted avg       0.92      0.92      0.92     25597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipe in pipelines:\n",
    "\n",
    "    precision = metrics.make_scorer(metrics.precision_score, pos_label=\"Yes\")\n",
    "    grid = GridSearchCV(pipe, parameters, cv=2, verbose=2, scoring=precision, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_precision = grid.best_score_\n",
    "    best_parameters = grid.best_params_\n",
    "  \n",
    "    print('\\n', \"Current Pipeline: \", pipe[1], '\\n')\n",
    "    print(\"Best precision: {}\".format(best_precision))\n",
    "    print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "    # Scoring Training data \n",
    "    print('\\n', 'Precision on training data: ', round(grid.score(X_train, y_train), 4))        # type: ignore\n",
    "    # Scoring Test data \n",
    "    print('Precision on test data: ', round(grid.score(X_test, y_test), 4), '\\n')      # type: ignore\n",
    "\n",
    "    print('Confusion Matrix : \\n' + str(metrics.confusion_matrix(y_test, grid.best_estimator_.predict(X_test))))\n",
    "\n",
    "    print(metrics.classification_report(y_test, grid.best_estimator_.predict(X_test))) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Hyperarameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_svc = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, min_df=5, max_df=0.85, stop_words=pt_stopwords, max_features=5000, ngram_range=(1, 3), use_idf=False)),\n",
    "                     ('svc', LinearSVC(class_weight='balanced'))])\n",
    "\n",
    "parameters_svc = {'svc__C':[0.1,1,10,100,1000],\n",
    "                'svc__loss':['hinge', 'squared_hinge'], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[CV] END ........................svc__C=0.1, svc__loss=hinge; total time=  13.2s\n",
      "[CV] END ........................svc__C=0.1, svc__loss=hinge; total time=  13.4s\n",
      "[CV] END ................svc__C=0.1, svc__loss=squared_hinge; total time=  13.9s\n",
      "[CV] END ..........................svc__C=1, svc__loss=hinge; total time=  14.1s\n",
      "[CV] END ..........................svc__C=1, svc__loss=hinge; total time=  14.1s\n",
      "[CV] END ................svc__C=0.1, svc__loss=squared_hinge; total time=  14.8s\n",
      "[CV] END ..................svc__C=1, svc__loss=squared_hinge; total time=  14.8s\n",
      "[CV] END ..................svc__C=1, svc__loss=squared_hinge; total time=  15.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .........................svc__C=10, svc__loss=hinge; total time=  18.7s\n",
      "[CV] END .........................svc__C=10, svc__loss=hinge; total time=  18.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........................svc__C=100, svc__loss=hinge; total time=  22.2s\n",
      "[CV] END .................svc__C=10, svc__loss=squared_hinge; total time=  23.6s\n",
      "[CV] END .................svc__C=10, svc__loss=squared_hinge; total time=  23.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ........................svc__C=100, svc__loss=hinge; total time=  23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................svc__C=100, svc__loss=squared_hinge; total time=  25.2s\n",
      "[CV] END ................svc__C=100, svc__loss=squared_hinge; total time=  26.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................svc__C=1000, svc__loss=hinge; total time=  23.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .......................svc__C=1000, svc__loss=hinge; total time=  24.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/nlp-test-neoway/env/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ...............svc__C=1000, svc__loss=squared_hinge; total time=  21.4s\n",
      "[CV] END ...............svc__C=1000, svc__loss=squared_hinge; total time=  21.4s\n",
      "\n",
      " Current Pipeline:  LinearSVC(class_weight='balanced') \n",
      "\n",
      "Best precision: 0.9698867112982654\n",
      "Best Parameters: {'svc__C': 0.1, 'svc__loss': 'hinge'}\n",
      "\n",
      " Precision on training data:  0.9726\n",
      "Precision on training data:  0.971 \n",
      "\n",
      "Confusion Matrix : \n",
      "[[ 6091   508]\n",
      " [ 1962 17036]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.76      0.92      0.83      6599\n",
      "         Yes       0.97      0.90      0.93     18998\n",
      "\n",
      "    accuracy                           0.90     25597\n",
      "   macro avg       0.86      0.91      0.88     25597\n",
      "weighted avg       0.92      0.90      0.91     25597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_grid = GridSearchCV(opt_svc, parameters_svc, cv=2, verbose=2, scoring=precision, n_jobs=-1, refit=True)\n",
    "svc_grid.fit(X_train, y_train)\n",
    "\n",
    "best_precision = svc_grid.best_score_\n",
    "best_parameters = svc_grid.best_params_\n",
    "# ADD PIPELINE NAME\n",
    "print('\\n', \"Current Pipeline: \", opt_svc[1], '\\n')\n",
    "print(\"Best precision: {}\".format(best_precision))\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "\n",
    "# Scoring Training data\n",
    "print('\\n', 'Precision on training data: ', round(svc_grid.score(X_train, y_train), 4))        # type: ignore\n",
    "# Scoring Test data\n",
    "print('Precision on test data: ', round(svc_grid.score(X_test, y_test), 4), '\\n')      # type: ignore\n",
    "\n",
    "print('Confusion Matrix : \\n' + str(metrics.confusion_matrix(y_test, svc_grid.best_estimator_.predict(X_test))))\n",
    "\n",
    "print(metrics.classification_report(y_test, svc_grid.best_estimator_.predict(X_test))) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../models/svc_pipeline_no_cleaning.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svc_grid.best_estimator_, '../../models/svc_pipeline_no_cleaning.pkl', compress = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearSVC' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Try to get importance of features\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# something like:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m svc_grid\u001b[39m.\u001b[39;49mbest_estimator_\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m\"\u001b[39;49m\u001b[39msvc\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mfeature_importances_  \u001b[39m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# Try to get importance of features\n",
    "# something like:\n",
    "svc_grid.best_estimator_.named_steps[\"svc\"].feature_importances_  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver score por categorias ou análise assim\n",
    "# métricas de classificação, \n",
    "# mudar threshold de classificação (0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre trained models (encoding)\n",
    "# pouco dados "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TO-DO) Create Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Make a new variable that is rating divided by number of reviews\n",
    "        X['review_text'] = X['review_title'] + ' ' + X['review_text']\n",
    "        X.drop_duplicates(inplace=True)\n",
    "        X.drop(columns=['review_title'], inplace=True)\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        X['review_text'] = X['review_text'].str.lower().copy()\n",
    "        X['review_text'] = X['review_text'].str.replace('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', 'negação', regex=True).copy()\n",
    "        regex_list = [r'www\\S+', r'http\\S+', r'@\\S+', r'#\\S+', r'[0-9]+', r'\\W', r'\\s+', r'[ \\t]+$']\n",
    "        for regex in regex_list:\n",
    "            X['review_text'] = X['review_text'].str.replace(regex, ' ', regex=True).copy()\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vars = ['review_text', 'review_title']\n",
    "\n",
    "text_pipeline = Pipeline([('data_cleaning', DataCleaning())])\n",
    "\n",
    "data_pipeline = ColumnTransformer(\n",
    "    [\n",
    "    ('text', text_pipeline, text_vars),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "num_transformed = data_pipeline.fit_transform(X) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC:\n",
    "\n",
    "pt_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "text_clf_lsvc = Pipeline([('tfidf', TfidfVectorizer(min_df=2, max_df=0.75, ngram_range=(1,3), stop_words=pt_stopwords)),\n",
    "                         ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "text_clf_lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'clf_C': [0.25, 0.5, 0.75, 1],\n",
    "                'clf_kernel': ['linear', 'rbf'],\n",
    "                'clf_gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "grid_search = GridSearchCV(estimator = text_clf_lsvc,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3,\n",
    "                           n_jobs = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\n",
    "print(\"Best Parameters:\", best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a prediction set\n",
    "predictions = text_clf_lsvc.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83c7377a892b124a9856191a5a9d20e3ff7659b1dab63cf9294e0f0bf41e709b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
